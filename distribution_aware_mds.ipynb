{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[<img src=\"https://colab.research.google.com/img/colab_favicon.ico\" alt=\"Open this notebook in Google Colab\" width=\"80\">](https://colab.research.google.com/github/GVourvachakis/Stochastic_Gamma_distribution_aware_MDS/blob/main/distribution_aware_mds.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVPmF6RHu73h",
        "outputId": "10bf39e6-a371-4f57-a83a-0431cf7b9252"
      },
      "outputs": [],
      "source": [
        "# Load domain libraries and dependencies\n",
        "import numpy as np\n",
        "#import matplotlib.pyplot as plt\n",
        "from typing import List, Dict, Any\n",
        "from time import time\n",
        "\n",
        "# Load California Housing Dataset\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "\n",
        "# Split the California housing dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Output the shapes of the resulting datasets\n",
        "print(\"California Housins Dataset:\")\n",
        "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
        "print(\"Testing set shape:\", X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcUe9Kz9vTvi"
      },
      "source": [
        "# MDS and Linear Regression Analysis\n",
        "\n",
        "$sampled \\ events $ ~ $  uni\\{1, dim(S)\\}$, where $S = sample \\ space$ (X.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nza4Pv7UvQIL"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import MDS\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eXDnNW4zvckA"
      },
      "outputs": [],
      "source": [
        "# Initialize StandardScaler for feature scaling\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Scale the features\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hygc-thZusdX"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "def mds_regression_analysis(X_train, y_train, X_test, y_test, feature_dim: List[int]) -> Dict[int, Dict[str, float]] | None:\n",
        "    \"\"\"\n",
        "    Perform MDS dimensionality reduction and linear regression analysis with memory optimization.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    X_train : array-like of shape (n_samples, n_features)\n",
        "        Training data\n",
        "    y_train : array-like of shape (n_samples,)\n",
        "        Target values for training\n",
        "    X_test : array-like of shape (n_samples, n_features)\n",
        "        Test data\n",
        "    y_test : array-like of shape (n_samples,)\n",
        "        Target values for test\n",
        "    feature_dim : List[int]\n",
        "        List of dimensions to reduce to\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    Dict : Dictionary containing results for each dimension\n",
        "    \"\"\"\n",
        "\n",
        "    # Dictionary to store results\n",
        "    results = {}\n",
        "\n",
        "    # Suppress convergence warnings\n",
        "    warnings.filterwarnings('ignore')\n",
        "\n",
        "    try:\n",
        "        for n_components in feature_dim:\n",
        "            print(f\"Processing {n_components}D reduction...\")\n",
        "\n",
        "            # Initialize MDS with memory-efficient parameters\n",
        "            mds = MDS(\n",
        "                n_components=n_components,\n",
        "                random_state=42,\n",
        "                n_init=1,              # Single initialization\n",
        "                max_iter=300,          # Limited iterations\n",
        "                n_jobs=1,             # Disable parallel processing\n",
        "                dissimilarity='euclidean',  # Use euclidean distance (faster)\n",
        "                eps=1e-3              # Slightly relaxed convergence criterion\n",
        "            )\n",
        "\n",
        "            # Process training data\n",
        "            print(\"Transforming training data...\")\n",
        "            X_train_mds = mds.fit_transform(X_train)\n",
        "\n",
        "            # Process test data\n",
        "            print(\"Transforming test data...\")\n",
        "            X_test_mds = mds.fit_transform(X_test)\n",
        "\n",
        "            # Train linear regression\n",
        "            print(\"Training linear regression...\")\n",
        "            lr = LinearRegression()\n",
        "            lr.fit(X_train_mds, y_train)\n",
        "\n",
        "            # Make predictions\n",
        "            print(\"Making predictions...\")\n",
        "            y_pred = lr.predict(X_test_mds)\n",
        "\n",
        "            # Calculate metrics\n",
        "            mse = mean_squared_error(y_test, y_pred)\n",
        "            mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "            # Store results\n",
        "            results[n_components] = {\n",
        "                'mse': mse,\n",
        "                'mae': mae\n",
        "            }\n",
        "\n",
        "            # Clear some memory\n",
        "            del X_train_mds, X_test_mds, y_pred\n",
        "\n",
        "            print(f\"Completed {n_components}D reduction\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "V1YcYZimewBq"
      },
      "outputs": [],
      "source": [
        "# Run the analysis with a smaller subset if needed\n",
        "def run_analysis(X_train, y_train, X_test, y_test, sample_size=None):\n",
        "    \"\"\"\n",
        "    Run the analysis with optional data sampling for memory efficiency.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    sample_size : int, optional\n",
        "        Number of samples to use. If None, uses full dataset.\n",
        "    \"\"\"\n",
        "    feature_dims = [2, 3]\n",
        "\n",
        "    start_time = time()\n",
        "\n",
        "    if sample_size is not None and sample_size < len(X_train):\n",
        "        # Random sampling for memory efficiency\n",
        "        np.random.seed(42)\n",
        "        train_idx = np.random.choice(len(X_train), sample_size, replace=False)\n",
        "        X_train_subset = X_train[train_idx]\n",
        "        y_train_subset = y_train[train_idx]\n",
        "\n",
        "        test_idx = np.random.choice(len(X_test), sample_size//4, replace=False)\n",
        "        X_test_subset = X_test[test_idx]\n",
        "        y_test_subset = y_test[test_idx]\n",
        "\n",
        "        print(f\"Using subset of {sample_size} training samples and {sample_size//4} test samples\")\n",
        "        metrics = mds_regression_analysis(X_train_subset, y_train_subset,\n",
        "                                       X_test_subset, y_test_subset, feature_dims)\n",
        "    else:\n",
        "        metrics = mds_regression_analysis(X_train, y_train, X_test, y_test, feature_dims)\n",
        "\n",
        "    if metrics is not None:\n",
        "        for dim in feature_dims:\n",
        "            print(f\"\\nResults for {dim}D reduction:\")\n",
        "            print(f\"MSE: {metrics[dim]['mse']:.4f}\")\n",
        "            print(f\"MAE: {metrics[dim]['mae']:.4f}\")\n",
        "\n",
        "    print(f\"Execution time: {time() - start_time:.2f} seconds\")\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OYeLs69mxI0E"
      },
      "outputs": [],
      "source": [
        "# Try running with a smaller sample size first\n",
        "# sample_size = 1000  # Adjust this value based on your available memory\n",
        "# metrics = run_analysis(X_train_scaled, y_train, X_test_scaled, y_test, sample_size=sample_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKhoiHP5fLRh"
      },
      "source": [
        "## Sampling with distribution matching/reconstruction awareness\n",
        "\n",
        "Data-specific observations from target's density histogram:\n",
        "\n",
        "1. The distribution is right-skewed\n",
        "2. There's a main peak around 1.5-2.0\n",
        "3. There's a long tail extending to about 5.0\n",
        "4. The distribution appears to be unimodal\n",
        "5. The sampling needs to maintain proper representation across all price ranges\n",
        "\n",
        "**Note**: *Kullback Leibler (KL) divergence* is generally more appropriate for comparing distributions that are similar in shape or location. In contrast, *Wasserstein distance* is robust to shifts in mass and can handle the shape and spread differences better, making it preferable for comparing distributions like the one described."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GEnuhqwc2JbX"
      },
      "outputs": [],
      "source": [
        "from utils import validate_sampling_distribution, perform_baseline_regression, uniform_sampling_mds_analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTYhPQFZkVa5",
        "outputId": "e0b7d5a9-dae0-4eaa-beb7-626dd35a6626"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyinstrument in /home/georgios-vourvachakis/anaconda3/lib/python3.11/site-packages (4.7.3)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install pyinstrument"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0woYbVHrmzA"
      },
      "source": [
        "The cell below is expected to run around **~ 20 min**.\n",
        "Employing 3 epochs/runs and obtaining profile statistics for the mds training routine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrHpTXBlgTon"
      },
      "outputs": [],
      "source": [
        "#Run the complete analysis\n",
        "warnings.filterwarnings('ignore')\n",
        "np.random.seed(42)\n",
        "\n",
        "# Perform analysis\n",
        "results = uniform_sampling_mds_analysis(X_train_scaled, y_train, X_test_scaled, y_test, sample_size=5_000, n_runs=1)\n",
        "\n",
        "# Validate sampling distribution\n",
        "validate_sampling_distribution(\n",
        "                                  results['sampling_validation']['y_full'],\n",
        "                                  results['sampling_validation']['y_sample'],\n",
        "                                  title = \"Target Variable Distribution: Full vs Sampled Dataset\",\n",
        "                                  dataset = \"California Housing\"\n",
        "                              )\n",
        "\n",
        "# Print comprehensive results\n",
        "print(\"\\nAnalysis Results:\")\n",
        "print(\"\\nBaseline Metrics (No dimensionality reduction):\")\n",
        "print(f\"MSE: {results['baseline_metrics']['mse']:.4f}\")\n",
        "print(f\"MAE: {results['baseline_metrics']['mae']:.4f}\")\n",
        "print(f\"R²: {results['baseline_metrics']['r2']:.4f}\")\n",
        "\n",
        "for dim in ['2D', '3D']:\n",
        "    print(f\"\\n{dim} MDS Reduction Metrics:\")\n",
        "    metrics = results['performance_metrics'][dim]\n",
        "    print(f\"MSE: {metrics['mse_mean']:.4f} ± {metrics['mse_std']:.4f}\")\n",
        "    print(f\"MAE: {metrics['mae_mean']:.4f} ± {metrics['mae_std']:.4f}\")\n",
        "    print(f\"R²: {metrics['r2_mean']:.4f} ± {metrics['r2_std']:.4f}\")\n",
        "\n",
        "print(\"\\nExecution Times:\")\n",
        "for key, value in results['timing_metrics'].items():\n",
        "    print(f\"{key}: {value:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSW8h-OtgggJ"
      },
      "source": [
        "# Single Run Uniform Sampling Benchmarks\n",
        "\n",
        "### Baseline Metrics (No Dimensionality Reduction)\n",
        "- **MSE**: 0.5559\n",
        "- **MAE**: 0.5332\n",
        "- **R²**: 0.5758\n",
        "\n",
        "### 2D MDS Reduction Metrics\n",
        "- **MSE**: 1.4021 ± 0.0527\n",
        "- **MAE**: 0.9330 ± 0.0183\n",
        "- **R²**: -0.0745 ± 0.0120\n",
        "\n",
        "### 3D MDS Reduction Metrics\n",
        "- **MSE**: 1.4745 ± 0.2832\n",
        "- **MAE**: 0.9360 ± 0.1055\n",
        "- **R²**: -0.1266 ± 0.1880\n",
        "\n",
        "### Execution Times\n",
        "- **2D**: 198.76 seconds\n",
        "- **3D**: 184.78 seconds\n",
        "- **Total**: 1124.21 seconds\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6zVkB3AbmO-"
      },
      "source": [
        "# Preconditioned Mini-batch stratified MDS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BzOen3l0rxrG"
      },
      "outputs": [],
      "source": [
        "from scipy import stats\n",
        "from scipy.stats import ks_2samp, wasserstein_distance\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "from pyinstrument import Profiler\n",
        "\n",
        "class DistributionAwareStratifiedSampler:\n",
        "    def __init__(self, n_bins=5):\n",
        "        self.n_bins = n_bins\n",
        "        self.kbd = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='quantile')\n",
        "        self.distribution_params = {}\n",
        "        self.distributions_info = {}  # Store detailed distribution information\n",
        "\n",
        "    def fit(self, y):\n",
        "        \"\"\"Fit the sampler to the target variable distribution.\"\"\"\n",
        "        self.y_binned = self.kbd.fit_transform(y.reshape(-1, 1)).flatten()\n",
        "        self.bin_distributions = {}\n",
        "        self.distributions_info = {}  # Reset distribution info\n",
        "\n",
        "        # Get bin boundaries for reporting\n",
        "        bin_edges = self.kbd.bin_edges_[0]\n",
        "\n",
        "        # Fit distribution for each bin\n",
        "        for bin_idx in range(self.n_bins):\n",
        "            bin_data = y[self.y_binned == bin_idx]\n",
        "            bin_range = f\"[{bin_edges[bin_idx]:.2f}, {bin_edges[bin_idx+1]:.2f})\"\n",
        "\n",
        "            # Find best distribution for this bin\n",
        "            best_dist, best_params, ks_stat = self._fit_distribution(bin_data)\n",
        "\n",
        "            self.bin_distributions[bin_idx] = {\n",
        "                'distribution': best_dist,\n",
        "                'params': best_params\n",
        "            }\n",
        "\n",
        "            # Store detailed information about the fit\n",
        "            if best_dist is not None:\n",
        "                self.distributions_info[bin_idx] = {\n",
        "                    'bin_range': bin_range,\n",
        "                    'distribution_name': best_dist.name,\n",
        "                    'parameters': best_params,\n",
        "                    'ks_statistic': ks_stat,\n",
        "                    'sample_size': len(bin_data)\n",
        "                }\n",
        "            else:\n",
        "                self.distributions_info[bin_idx] = {\n",
        "                    'bin_range': bin_range,\n",
        "                    'distribution_name': 'fallback_uniform',\n",
        "                    'parameters': None,\n",
        "                    'ks_statistic': None,\n",
        "                    'sample_size': len(bin_data)\n",
        "                }\n",
        "\n",
        "        return self\n",
        "\n",
        "    def _fit_distribution(self, data):\n",
        "        \"\"\"Find the best-fitting distribution for the given data.\"\"\"\n",
        "        distributions = [\n",
        "            (stats.gamma, ['a', 'loc', 'scale']),\n",
        "            (stats.lognorm, ['s', 'loc', 'scale']),\n",
        "            (stats.norm, ['loc', 'scale']),\n",
        "            (stats.beta, ['a', 'b', 'loc', 'scale'])\n",
        "        ]\n",
        "\n",
        "        best_fit = None\n",
        "        best_ks = float('inf')\n",
        "        best_params = None\n",
        "\n",
        "        for distribution, param_names in distributions:\n",
        "            try:\n",
        "                # Fit distribution\n",
        "                params = distribution.fit(data)\n",
        "                # Generate samples for KS test\n",
        "                samples = distribution.rvs(*params, size=len(data))\n",
        "                # Perform KS test\n",
        "                ks_stat, _ = ks_2samp(data, samples)\n",
        "\n",
        "                if ks_stat < best_ks:\n",
        "                    best_ks = ks_stat\n",
        "                    best_fit = distribution\n",
        "                    best_params = dict(zip(param_names, params))\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        return best_fit, best_params, best_ks if best_fit is not None else None\n",
        "\n",
        "    def sample(self, y, sample_size):\n",
        "        \"\"\"Generate stratified samples using fitted distributions.\"\"\"\n",
        "        samples_per_bin = sample_size // self.n_bins\n",
        "        sampled_indices = []\n",
        "\n",
        "        for bin_idx in range(self.n_bins):\n",
        "            bin_indices = np.where(self.y_binned == bin_idx)[0]\n",
        "            bin_dist_info = self.bin_distributions[bin_idx]\n",
        "\n",
        "            if bin_dist_info['distribution'] is None:\n",
        "                # Fallback to random sampling if distribution fitting failed\n",
        "                selected_indices = np.random.choice(\n",
        "                    bin_indices,\n",
        "                    size=min(samples_per_bin, len(bin_indices)),\n",
        "                    replace=False\n",
        "                )\n",
        "            else:\n",
        "                # Generate samples from fitted distribution\n",
        "                dist = bin_dist_info['distribution']\n",
        "                params = bin_dist_info['params']\n",
        "\n",
        "                # Generate more samples than needed for better matching\n",
        "                oversample_factor = 2\n",
        "                generated_samples = dist.rvs(\n",
        "                    **params,\n",
        "                    size=samples_per_bin * oversample_factor\n",
        "                )\n",
        "\n",
        "                # Find closest matches in the original data\n",
        "                bin_data = y[bin_indices]\n",
        "                selected_indices = []\n",
        "\n",
        "                for target_value in generated_samples[:samples_per_bin]:\n",
        "                    closest_idx = bin_indices[\n",
        "                        np.abs(bin_data - target_value).argmin()\n",
        "                    ]\n",
        "                    if closest_idx not in selected_indices:\n",
        "                        selected_indices.append(closest_idx)\n",
        "\n",
        "                # If we couldn't find enough unique matches, fall back to random sampling\n",
        "                remaining = samples_per_bin - len(selected_indices)\n",
        "                if remaining > 0:\n",
        "                    available_indices = list(\n",
        "                        set(bin_indices) - set(selected_indices)\n",
        "                    )\n",
        "                    if available_indices:\n",
        "                        additional_indices = np.random.choice(\n",
        "                            available_indices,\n",
        "                            size=min(remaining, len(available_indices)),\n",
        "                            replace=False\n",
        "                        )\n",
        "                        selected_indices.extend(additional_indices)\n",
        "\n",
        "            sampled_indices.extend(selected_indices)\n",
        "\n",
        "        return np.array(sampled_indices)\n",
        "\n",
        "    def get_distribution_summary(self):\n",
        "        \"\"\"Return a formatted summary of the fitted distributions.\"\"\"\n",
        "        summary = \"Distribution Fitting Summary:\\n\" + \"=\"*50 + \"\\n\"\n",
        "        for bin_idx, info in self.distributions_info.items():\n",
        "            summary += f\"\\nBin {bin_idx} ({info['bin_range']}):\\n\"\n",
        "            summary += f\"  Distribution: {info['distribution_name']}\\n\"\n",
        "            summary += f\"  Sample Size: {info['sample_size']}\\n\"\n",
        "            if info['ks_statistic'] is not None:\n",
        "                summary += f\"  KS Statistic: {info['ks_statistic']:.4f}\\n\"\n",
        "            if info['parameters'] is not None:\n",
        "                summary += \"  Parameters:\\n\"\n",
        "                for param_name, param_value in info['parameters'].items():\n",
        "                    summary += f\"    {param_name}: {param_value:.4f}\\n\"\n",
        "        return summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HU5fMPuGsXdj"
      },
      "outputs": [],
      "source": [
        "def optimized_stratified_mds_analysis(X_train, y_train, X_test, y_test, \\\n",
        "                                      sample_size=5_000, n_runs = 3) -> Dict[str, Dict[str, Any]]:\n",
        "\n",
        "    \"\"\"\n",
        "    Perform MDS analysis using distribution-aware stratified sampling with validation metrics.\n",
        "    \"\"\"\n",
        "\n",
        "    results = {\n",
        "        'sampling_validation': {},\n",
        "        'performance_metrics': {'2D': {}, '3D': {}},\n",
        "        'timing_metrics': {},\n",
        "        'baseline_metrics': {},\n",
        "        'distribution_info': {}  # New field for distribution information\n",
        "    }\n",
        "\n",
        "    start_time = time()\n",
        "\n",
        "    # Initialize and fit the distribution-aware sampler\n",
        "    sampler = DistributionAwareStratifiedSampler(n_bins=5)\n",
        "    sampler.fit(y_train)\n",
        "\n",
        "    # Store distribution information\n",
        "    results['distribution_info']['summary'] = sampler.get_distribution_summary()\n",
        "    results['distribution_info']['details'] = sampler.distributions_info\n",
        "\n",
        "    results_2d = {'mse': [], 'mae': [], 'r2': []}\n",
        "    results_3d = {'mse': [], 'mae': [], 'r2': []}\n",
        "\n",
        "    # Initialize profiler\n",
        "    profiler = Profiler()\n",
        "\n",
        "    n_runs = n_runs\n",
        "    for run in tqdm(range(n_runs), desc=\"Overall runs\"):\n",
        "        # Generate samples using the distribution-aware sampler\n",
        "        train_indices = sampler.sample(y_train, sample_size)\n",
        "\n",
        "        # Sample the data\n",
        "        X_train_sample = X_train[train_indices]\n",
        "        y_train_sample = y_train[train_indices]\n",
        "\n",
        "        # Sample test set\n",
        "        test_size = len(y_test) // 4\n",
        "        test_indices = np.random.choice(len(y_test), size=test_size, replace=False)\n",
        "        X_test_sample = X_test[test_indices]\n",
        "        y_test_sample = y_test[test_indices]\n",
        "\n",
        "        # Store sampling validation data for first run\n",
        "        if run == 0:\n",
        "            results['sampling_validation'] = {\n",
        "                'y_full': y_train,\n",
        "                'y_sample': y_train_sample,\n",
        "                'sample_size': len(y_train_sample)\n",
        "            }\n",
        "\n",
        "        # Profile MDS transformation for both 2D and 3D\n",
        "        if profiler.is_running:\n",
        "            profiler.stop() # maintaining asynchronous mode\n",
        "        profiler.start()  # Start profiling\n",
        "\n",
        "        # Process both 2D and 3D reductions\n",
        "        for n_components in [2, 3]:\n",
        "            dim_start_time = time()\n",
        "            tqdm.write(f\"Run {run + 1}/{n_runs}: Processing {n_components}D reduction...\")\n",
        "\n",
        "            # MDS transformation\n",
        "            mds = MDS(\n",
        "                n_components=n_components,\n",
        "                random_state= 42 + run, # switch between pseudo-random microstates\n",
        "                n_init=1,\n",
        "                max_iter=300,\n",
        "                n_jobs=-1, # multi-core execution [use all available CPU cores]\n",
        "                dissimilarity='euclidean',\n",
        "                eps=1e-3\n",
        "            )\n",
        "\n",
        "            # Transform data\n",
        "            X_train_mds = mds.fit_transform(X_train_sample)\n",
        "            X_test_mds = mds.fit_transform(X_test_sample)\n",
        "\n",
        "            # Fit regression and predict\n",
        "            lr = LinearRegression()\n",
        "            lr.fit(X_train_mds, y_train_sample)\n",
        "            y_pred = lr.predict(X_test_mds)\n",
        "\n",
        "            # Calculate metrics\n",
        "            mse = mean_squared_error(y_test_sample, y_pred)\n",
        "            mae = mean_absolute_error(y_test_sample, y_pred)\n",
        "            r2 = r2_score(y_test_sample, y_pred)\n",
        "\n",
        "            # Store results\n",
        "            metrics_dict = results_2d if n_components == 2 else results_3d\n",
        "            metrics_dict['mse'].append(mse)\n",
        "            metrics_dict['mae'].append(mae)\n",
        "            metrics_dict['r2'].append(r2)\n",
        "\n",
        "            # Store timing for first run\n",
        "            if run == 0:\n",
        "                results['timing_metrics'][f'{n_components}D'] = time() - dim_start_time\n",
        "\n",
        "        profiler.stop()  # Stop profiling\n",
        "\n",
        "    # Print profiling results\n",
        "    profiler.print()\n",
        "\n",
        "    # Calculate baseline metrics\n",
        "    print(\"\\nCalculating baseline metrics...\")\n",
        "    results['baseline_metrics'] = perform_baseline_regression(X_train, y_train, X_test, y_test) # by \"utils\" module\n",
        "\n",
        "\n",
        "    # Calculate and store final performance metrics\n",
        "    for dim, res in [('2D', results_2d), ('3D', results_3d)]:\n",
        "        results['performance_metrics'][dim] = {\n",
        "            'mse_mean': np.mean(res['mse']),\n",
        "            'mse_std': np.std(res['mse']),\n",
        "            'mae_mean': np.mean(res['mae']),\n",
        "            'mae_std': np.std(res['mae']),\n",
        "            'r2_mean': np.mean(res['r2']),\n",
        "            'r2_std': np.std(res['r2'])\n",
        "        }\n",
        "\n",
        "    # Store total execution time\n",
        "    results['timing_metrics']['total'] = time() - start_time\n",
        "\n",
        "    # Print distribution summary\n",
        "    #print(\"\\nDistribution Fitting Results:\")\n",
        "    print(results['distribution_info']['summary'])\n",
        "\n",
        "    # Access detailed distribution parameters for specific bins\n",
        "    for bin_idx, info in results['distribution_info']['details'].items():\n",
        "        print(f\"\\nBin {bin_idx} distribution: {info['distribution_name']}\")\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJYiga0GPmHg"
      },
      "source": [
        "The cell below is expected to run around **~ 20 min**.\n",
        "Employing 3 epochs/runs, obtaining profile statistics for the mds training routine, running for the optimal sampling distribution of each bin."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kOzlZSNsLvi",
        "outputId": "b9317963-4eb8-4722-fba4-14c6e200272c"
      },
      "outputs": [],
      "source": [
        "# Perform analysis\n",
        "results = optimized_stratified_mds_analysis(X_train, y_train, X_test, y_test, n_runs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTGVLBslFfTS"
      },
      "source": [
        "## **Characterization of Gamma-Beta distribution and KS score**\n",
        "\n",
        "**Distribution: Beta**: Indicates that the beta distribution is the underlying statistical model used to fit the data.\n",
        "\n",
        "**Sample Size**: Specifies the number of observations or data points used to fit the beta distribution.\n",
        "\n",
        "**KS Statistic**: The Kolmogorov-Smirnov (KS) statistic measures the maximum distance between the empirical cumulative distribution function (CDF) of the sample data and the CDF of the fitted beta distribution. A smaller KS statistic suggests that the beta distribution is a good fit for the data.\n",
        "\n",
        "Parameters:\n",
        "* **a**: The first shape parameter of the beta distribution, which influences the distribution's skewness and kurtosis.\n",
        "\n",
        "* **b**: The second shape parameter of the beta distribution, similarly affecting its shape.\n",
        "\n",
        "* **loc**: The location parameter, which shifts the distribution along the x-axis. In this case, it adjusts the lower bound of the beta distribution.\n",
        "\n",
        "* **scale**: The scale parameter, which stretches or compresses the distribution, determining the width of the range over which the beta distribution is defined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 966
        },
        "id": "bsK4dVsAxBtb",
        "outputId": "98fe332a-1d1a-4c22-beb5-f55ba26cfbfc"
      },
      "outputs": [],
      "source": [
        "# Suppress warnings\n",
        "#warnings.filterwarnings('ignore')  # Ignore warnings for a cleaner output\n",
        "np.random.seed(42)  # Ensure reproducibility\n",
        "\n",
        "# Validate the sampling distribution (by \"utils\" module)\n",
        "validate_sampling_distribution(\n",
        "    y_full=results['sampling_validation'].get('y_full', []),\n",
        "    y_sample=results['sampling_validation'].get('y_sample', []),\n",
        "    title=\"Target Variable Distribution: Full vs Sampled Dataset\",\n",
        "    dataset = \"California Housing\"\n",
        ")\n",
        "\n",
        "# Display comprehensive analysis results\n",
        "print(\"\\nAnalysis Results:\")\n",
        "\n",
        "# Print baseline metrics for comparison (without dimensionality reduction)\n",
        "baseline_metrics = results.get('baseline_metrics', {})\n",
        "\n",
        "print(\"\\nBaseline Metrics (No dimensionality reduction):\")\n",
        "print(f\"MSE: {baseline_metrics.get('mse', float('nan')):.4f}\")\n",
        "print(f\"MAE: {baseline_metrics.get('mae', float('nan')):.4f}\")\n",
        "print(f\"R²: {baseline_metrics.get('r2', float('nan')):.4f}\")\n",
        "\n",
        "# Print performance metrics for each MDS reduction (2D and 3D)\n",
        "for dim in ['2D', '3D']:\n",
        "    print(f\"\\n{dim} MDS Reduction Metrics:\")\n",
        "    performance_metrics = results['performance_metrics'].get(dim, {})\n",
        "    print(f\"MSE: {performance_metrics.get('mse_mean', float('nan')):.4f} ± {performance_metrics.get('mse_std', float('nan')):.4f}\")\n",
        "    print(f\"MAE: {performance_metrics.get('mae_mean', float('nan')):.4f} ± {performance_metrics.get('mae_std', float('nan')):.4f}\")\n",
        "    print(f\"R²: {performance_metrics.get('r2_mean', float('nan')):.4f} ± {performance_metrics.get('r2_std', float('nan')):.4f}\")\n",
        "\n",
        "# Print execution times for dimensionality reductions and the total analysis\n",
        "print(\"\\nExecution Times:\")\n",
        "timing_metrics = results.get('timing_metrics', {})\n",
        "for key, value in timing_metrics.items():\n",
        "    print(f\"{key}: {value:.2f} seconds\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
